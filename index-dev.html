<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge">
    	<meta name="viewport" content="width=device-width, initial-scale=1.0">
    	<meta name="keywords" content="statistics,research,math,phd,high dimensional,compressed sensing,bayesian networks">
    	<meta name="description" content="Personal homepage of Bryon Aragam, PhD candidate in Statistics at UCLA.">
	    <meta name="author" content="Bryon Aragam">
		<title>Bryon Aragam // UCLA Statistics</title>
		
		<!-- CSS -->
		<link rel="stylesheet" href="./frameworks/bootstrap/css/bootstrap.min.css"  type="text/css" />
		<link rel="stylesheet" href="main.css"  type="text/css" />
		<link href="http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300,700%7CRoboto:100,300,400" rel="stylesheet" type="text/css">
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">

		<!-- JAVASCRIPT LIBRARIES -->
		<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>
		<script src="http://code.jquery.com/color/jquery.color-2.1.2.min.js"></script>
		<script src="./frameworks/bootstrap/js/bootstrap.min.js" type="text/javascript"></script>

		<!-- Respond.js for IE8 support of HTML5 elements and media queries -->
	    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
	    <!--[if lt IE 9]>
	    	<script src="./frameworks/respond.min.js"></script>
	    <![endif]-->

	    <!-- MY JAVASCRIPT -->
	    <script type="text/javascript">
	    	$(document).ready(function(){
	    		$("body").hide();
	    	});
	    </script>
		<script src="hide.js"></script>
		<script src="preloader.js"></script>
	</head>
	
	<body>
		<div class="content-hidden active-day" id="bg"></div>
		<div class="container" id="my-main-container">
			
			<div id="bg-layer">

				<div class="row"> <!-- BEGIN ROW #1 -->
					<nav class="navbar navbar-default navbar-fixed-top" role="navigation" id="my-navbar-xs">
		        		<div class="container-fluid">		
				        		
				        		<div class="navbar-header my-main-heading">
									<button type="button" class="navbar-toggle visible-xs" data-toggle="collapse" data-target="#my-nav-list-xs" id="my-nav-button">
								        <span class="sr-only">Toggle navigation</span>
								        <span class="icon-bar"></span>
								        <span class="icon-bar"></span>
								        <span class="icon-bar"></span>
								    </button>
								    <a class="my-main-title hidden-xs" id="my-navbar-header" href="#"> // (n)ikhyl (b)ryon (a)ragam</a>
								    <a class="my-main-title visible-xs" id="my-navbar-header-xs" href="#"> // (b)ryon (a)ragam</a>
								</div>

								<div class="collapse navbar-collapse" id="my-nav-list-xs">
									  <ul class="nav navbar-nav visible-xs">
										<li><a class="hover-day-clean" id="research-xs" href="#">research</a></li>
										<li><a class="hover-day-clean" id="teaching-xs" href="#">teaching</a></li>
										<li><a class="hover-day-clean" id="software-xs" href="#">software</a></li>
										<li><a class="hover-day-clean" id="consulting-xs" href="#">consulting</a></li>
										<li><a class="hover-day-clean" id="about-xs" href="#">about</a></li>
									  </ul>
								</div>
								
			        	</div>
	        		</nav>
				</div> <!-- END ROW #1 -->

				<div class="row"> <!-- BEGIN ROW #2 -->
					<div class="col-sm-3 hidden-xs">
						<ul class="nav" id="my-navbar">
							<li><a class="hover-day-clean" id="research" href="#">research</a></li>
							<li><a class="hover-day-clean" id="teaching" href="#">teaching</a></li>
							<li><a class="hover-day-clean" id="software" href="#">software</a></li>
							<li><a class="hover-day-clean" id="consulting" href="#">consulting</a></li>
							<li><a class="hover-day-clean" id="about" href="#">about</a></li>
							<!--<li>
								<div class="padding:180px;">
									<a class="hidden-xs hover-day-clean" style="float:left; margin-left:18px" id="linkedin" href="http://www.linkedin.com/in/test"><i class="fa fa-linkedin fa-1x"></i></a> 
									<a class="hidden-xs" style="float:left; margin-left:18px" href="https://github.com/test"><i class="fa fa-github fa-1x"></i></a> 
									<a class="hidden-xs" style="float:left; margin-left:18px" id="email-xs" href="mailto:test@stat.ucla.edu"><i class="fa fa-envelope-o fa-1x"></i> </a> 
								</div>
							</li>-->
						</ul>
					</div>

					<div class="col-xs-12 col-sm-9 my-main-content">
						<div class="my-content" id="intro-content">
							<p class="my-hidden-heading">// about</p>
							<div class="content-wrapper">
								<p>Lecturer in the Department of Statistics at UCLA.</p>
							</div>
								
							<p class="my-subject-heading">// research interests</p>
							<ul class="list-wrapper">
								<li>Machine learning</li>
								<li>Graphical models</li>
								<li>Computational statistics</li>
								<li>High-dimensional statistics</li>
								<li>Unsupervised learning</li>
							</ul>
							
							<p class="my-subject-heading">// publications and preprints</p>
							<div class="panel-group" id="intro-accordion" role="tablist" aria-multiselectable="true">
								<div> <!-- BEGIN ENTRY -->
									<div class="panel-heading" role="tab" id="header-highdimdag-intro">
										<p class="article-ref">
											Aragam, B., Amini, A. and Zhou, Q. (2015). <a class="collapsed my-link" data-toggle="collapse" data-parent="#intro-accordion" href="#collapse-highdimdag-intro" aria-expanded="false" aria-controls="collapse-highdimdag-intro">Penalized least squares estimation of directed acyclic graphs.</a> (<a class="my-link" href="http://arxiv.org/pdf/1401.0852v2.pdf">preprint</a>)
										</p>
									</div>
									<div id="collapse-highdimdag-intro" class="panel-collapse collapse" role="tabpanel" aria-labelledby="header-highdimdag-intro">
										<div> <!--- Used to be a wrapper for class="panel-body"; no longer functional -->
											<div class="abstract-panel content-wrapper">
												<p>
													We consider the problem of estimating a directed graphical model for a multivariate normal distribution. Our main results establish nonasymptotic deviation bounds on the estimation error, sparsity bounds, and model selection consistency for a penalized least squares estimator that allow for high-dimensional data with p >> n. The proof relies on interpreting the graphical model as a recursive linear structural equation model, which reduces the estimation problem to a series of tractable neighbourhood regression problems. Our results apply to a wide variety of practical situations that allow for general dependencies in the data as well as general regularizers including the MCP, SCAD, l0 and l1.
												</p>

												<p>
													<span class="heavy">Keywords:</span> Graphical modeling, high-dimensional statistics, regularization, directed acyclic graphs, structural equations, sparse regression
												</p>

												<p>
													<a class="my-link" href="http://arxiv.org/abs/1401.0852">Link to arXiv</a>
												</p>
											</div>
										</div>
									</div>
								</div> <!-- END ENTRY -->

								<div> <!-- BEGIN ENTRY -->
									<div class="panel-heading" role="tab" id="header-ccdr-intro">
										<p class="article-ref">
									        Aragam, B. and Zhou, Q. (2015). <a class="collapsed my-link" data-toggle="collapse" data-parent="#intro-accordion" href="#collapse-ccdr-intro" aria-expanded="false" aria-controls="collapse-ccdr-intro">Concave penalized estimation of sparse Gaussian Bayesian networks.</a> Journal of Machine Learning Research, in press. (<a class="my-link" href="http://arxiv.org/pdf/1401.0852v2.pdf">preprint</a>)
								        </p>
									</div>
									<div id="collapse-ccdr-intro" class="panel-collapse collapse" role="tabpanel" aria-labelledby="header-ccdr-intro">
										<div> <!--- Used to be a wrapper for class="panel-body"; no longer functional -->
											<div class="abstract-panel content-wrapper">
												<p>
										        	We develop a penalized likelihood estimation framework to learn the structure of Gaussian Bayesian networks from observational data. In contrast to recent methods which accelerate the learning problem by restricting the search space, our main contribution is a fast algorithm for score-based structure learning which does not restrict the search space in any way and works on high-dimensional data sets with thousands of variables. Our use of concave regularization, as opposed to the more popular l0 (e.g. BIC) penalty, is new. Moreover, we provide theoretical guarantees which generalize existing asymptotic results when the underlying distribution is Gaussian. Most notably, our framework does not require the existence of a so-called faithful DAG representation, and as a result, the theory must handle the inherent nonidentifiability of the estimation problem in a novel way. Finally, as a matter of independent interest, we provide a comprehensive comparison of our approach to several standard structure learning methods using open-source packages developed for the R language. Based on these experiments, we show that our algorithm obtains higher sensitivity with comparable false discovery rates for high-dimensional data and scales efficiently as the number of nodes increases. In particular, the total runtime for our method to generate a solution path of 20 estimates for DAGs with 8000 nodes is around one hour.
										        </p>
												
												<p>
													<span class="heavy">Keywords:</span> Bayesian networks, concave penalization, directed acyclic graphs, coordinate descent, nonconvex optimization
												</p>

										        <p>
										        	<a class="my-link" href="http://arxiv.org/abs/1401.0852">Link to arXiv</a>
										        </p>
											</div>
										</div>
									</div>
								</div> <!-- END ENTRY -->
							</div> <!-- END ACCORDION DIV -->
							
						</div> <!-- END INTRO-CONTENT DIV -->

						<div class="my-content" id="research-content">
							<p class="my-hidden-heading">// research</p>
							<div class="content-wrapper">
								<p>My main interests involve problems at the intersection of high-dimensional statistics and machine learning, with a focus on developing algorithms with sound theoretical guarantees. Some of my general interests include:</p>

								<ul class="extra-indent">
									<li>Machine learning / graphical modeling / unsupervised learning</li>
									<li>Computational statistics / distributed learning / high performance computing</li>
									<li>High-dimensional statistics / compressed sensing</li>
									<li>Sparse modeling / low-rank approximation / inverse problems</li>
									<li>Bayesian networks / causal inference</li>
									<li>Decision theory / social choice theory</li>
								</ul>
							</div>
								
							<!-- PREPRINTS -->
							<p class="my-subject-heading">// preprints</p>
							<div class="panel-group" id="preprints-accordion" role="tablist" aria-multiselectable="true">
								<div> <!-- BEGIN ENTRY -->
									<div class="panel-heading" role="tab" id="header-highdimdag-preprints">
										<p class="article-ref">
											Aragam, B., Amini, A. and Zhou, Q. (2015). <a class="collapsed my-link" data-toggle="collapse" data-parent="#preprints-accordion" href="#collapse-highdimdag-preprints" aria-expanded="false" aria-controls="collapse-highdimdag-preprints">Penalized least squares estimation of directed acyclic graphs.</a> (<a class="my-link" href="http://arxiv.org/pdf/1401.0852v2.pdf">preprint</a>)
										</p>
									</div>
									<div id="collapse-highdimdag-preprints" class="panel-collapse collapse" role="tabpanel" aria-labelledby="header-highdimdag-preprints">
										<div> <!--- Used to be a wrapper for class="panel-body"; no longer functional -->
											<div class="abstract-panel content-wrapper">
												<p>
													We consider the problem of estimating a directed graphical model for a multivariate normal distribution. Our main results establish nonasymptotic deviation bounds on the estimation error, sparsity bounds, and model selection consistency for a penalized least squares estimator that allow for high-dimensional data with p >> n. The proof relies on interpreting the graphical model as a recursive linear structural equation model, which reduces the estimation problem to a series of tractable neighbourhood regression problems. Our results apply to a wide variety of practical situations that allow for general dependencies in the data as well as general regularizers including the MCP, SCAD, l0 and l1.
												</p>

												<p>
													<span class="heavy">Keywords:</span> Graphical modeling, high-dimensional statistics, regularization, directed acyclic graphs, structural equations, sparse regression
												</p>

												<p>
													<a class="my-link" href="http://arxiv.org/abs/1401.0852">Link to arXiv</a>
												</p>
											</div>
										</div>
									</div>
								</div> <!-- END ENTRY -->
							</div> <!-- END ACCORDION DIV -->

							<!-- PUBLICATIONS -->
							<p class="my-subject-heading">// publications</p>
							<div class="panel-group" id="pubs-accordion" role="tablist" aria-multiselectable="true">
								<div> <!-- BEGIN ENTRY -->
									<div class="panel-heading" role="tab" id="header-ccdr-pubs">
										<p class="article-ref">
									        Aragam, B. and Zhou, Q. (2015). <a class="collapsed my-link" data-toggle="collapse" data-parent="#pubs-accordion" href="#collapse-ccdr-pubs" aria-expanded="false" aria-controls="collapse-ccdr-pubs">Concave penalized estimation of sparse Gaussian Bayesian networks.</a> Journal of Machine Learning Research, in press. (<a class="my-link" href="http://arxiv.org/pdf/1401.0852v2.pdf">preprint</a>)
								        </p>
									</div>
									<div id="collapse-ccdr-pubs" class="panel-collapse collapse" role="tabpanel" aria-labelledby="header-ccdr-pubs">
										<div> <!--- Used to be a wrapper for class="panel-body"; no longer functional -->
											<div class="abstract-panel content-wrapper">
												<p>
										        	We develop a penalized likelihood estimation framework to learn the structure of Gaussian Bayesian networks from observational data. In contrast to recent methods which accelerate the learning problem by restricting the search space, our main contribution is a fast algorithm for score-based structure learning which does not restrict the search space in any way and works on high-dimensional data sets with thousands of variables. Our use of concave regularization, as opposed to the more popular l0 (e.g. BIC) penalty, is new. Moreover, we provide theoretical guarantees which generalize existing asymptotic results when the underlying distribution is Gaussian. Most notably, our framework does not require the existence of a so-called faithful DAG representation, and as a result, the theory must handle the inherent nonidentifiability of the estimation problem in a novel way. Finally, as a matter of independent interest, we provide a comprehensive comparison of our approach to several standard structure learning methods using open-source packages developed for the R language. Based on these experiments, we show that our algorithm obtains higher sensitivity with comparable false discovery rates for high-dimensional data and scales efficiently as the number of nodes increases. In particular, the total runtime for our method to generate a solution path of 20 estimates for DAGs with 8000 nodes is around one hour.
										        </p>
												
												<p>
													<span class="heavy">Keywords:</span> Bayesian networks, concave penalization, directed acyclic graphs, coordinate descent, nonconvex optimization
												</p>

										        <p>
										        	<a class="my-link" href="http://arxiv.org/abs/1401.0852">Link to arXiv</a>
										        </p>
											</div>
										</div>
									</div>
								</div> <!-- END ENTRY -->
							</div> <!-- END ACCORDION DIV -->

							<!-- BEGIN THESES -->
							<p class="my-subject-heading">// dissertation and theses</p>
							<div class="panel-group" id="thesis-accordion" role="tablist" aria-multiselectable="true">
								<div> <!-- BEGIN ENTRY -->
									<div class="panel-heading" role="tab" id="header-dagthesis-thesis">
										<p class="article-ref">
									        <a class="collapsed my-link" data-toggle="collapse" data-parent="#thesis-accordion" href="#collapse-dagthesis-thesis" aria-expanded="false" aria-controls="collapse-dagthesis-thesis">Structure Learning of Linear Bayesian Networks in High-Dimensions</a>, PhD Thesis (joint work with Q. Zhou and A. Amini)
								        </p>
									</div>
									<div id="collapse-dagthesis-thesis" class="panel-collapse collapse" role="tabpanel" aria-labelledby="header-dagthesis-thesis">
										<div> <!--- Used to be a wrapper for class="panel-body"; no longer functional -->
											<div class="abstract-panel content-wrapper">
												<p>
										        	Research into graphical models is a rapidly developing enterprise, garnering significant interest from both the statistics and machine learning communities. A parallel thread in both communities has been the study of low-dimensional structures in high-dimensional models where $p\gg n$. Recently, there has been a surge of interest in connecting these threads in order to understand the behaviour of graphical models in high-dimensions. Due to their relative simplicity, undirected models such as the Gaussian graphical model and Ising models have received most of the attention, whereas directed graphical models have received comparatively little attention. An important yet largely unresolved class of directed graphical models are Bayesian networks, or directed acyclic graphs (DAGs). These models have a wide variety of applications in aritificial intelligence, machine learning, genetics, and computer vision, but estimation of Bayesian networks in high-dimensions is not well-understood. The main focus of this dissertation is to address some fundamental questions about these models in high-dimensions.
												</p>
												<p>
													The primary goal is to develop both algorithms and theory for estimating continuous, linear Bayesian networks, capable of handling modern high-dimensional problems. Motivated by problems from the regression literature, we show how to adapt recent work in sparse learning and nonconvex optimization to the structure learning problem for Bayesian networks in order to estimate DAGs with several thousand nodes. We draw an explicit connection between linear Bayesian networks and so-called neighbourhood regression problems and show how this can be exploited in order to derive nonasymptotic performance bounds for penalized least squares estimators of directed graphical models.
												</p>
												<p>
													On the algorithmic side, we develop a method for estimating Gaussian Bayesian networks based on convex reparametrization and cyclic coordinate descent. In contrast to recent methods which accelerate the learning problem by restricting the search space, we propose a method for score-based structure learning which does not restrict the search space. We do not require the existence of a so-called faithful DAG representation, and as a result, our methodology must handle the inherent nonidentifiability of the estimation problem in a novel way. On the theoretical side, we provide (a) Finite-dimensional performance guarantees for local minima of the resulting nonconvex program, and (b) A general high-dimensional framework for global minima of the nonconvex program. Both the algorithms and theory apply to a general class of regularizers, including the MCP, SCAD, $\ell_1$ and $\ell_0$ penalties. Finally, as a matter of independent interest, we provide a comprehensive comparison of our approach to several standard structure learning methods using open-source packages developed for the \texttt{R} language.
										        </p>
												
												<p>
													<span class="heavy">Keywords:</span> Bayesian networks, high-dimensional statistics, graphical models, sparse regression, concave regularization, nonconvex optimization
												</p>
											</div>
										</div>
									</div>
								</div> <!-- END ENTRY -->

								<div> <!-- BEGIN ENTRY -->
									<div class="panel-heading" role="tab" id="header-lorentz-thesis">
										<p class="article-ref">
									        <a class="collapsed my-link" data-toggle="collapse" data-parent="#thesis-accordion" href="#collapse-lorentz-thesis" aria-expanded="false" aria-controls="collapse-lorentz-thesis">Volume Comparison with Integral Bounds in Lorentz Manifolds</a>, Undergraduate Thesis (joint work with J. Corvino, A. Karl, and A. Rochford)
								        </p>
									</div>
									<div id="collapse-lorentz-thesis" class="panel-collapse collapse" role="tabpanel" aria-labelledby="header-lorentz-thesis">
										<div> <!--- Used to be a wrapper for class="panel-body"; no longer functional -->
											<div class="abstract-panel content-wrapper">
												<p>
										        	Ten years ago, Ehrlich and Sanchez produced a pointwise statement of the classical Bishop volume comparison theorem for so-called SCLV subsets of the causal future in a Lorentz manifold, while Petersen and Wei developed and proved an integral version for Riemannian manifolds. We apply Peterson and Wei's method to the SCLV sets, and verify that two essential differential equations from the Riemannian proof extend to the Lorentz setting. As a result, we obtain a volume comparison theorem for Lorentz manifolds with integral, rather than pointwise, bounds. We also brie􏱭y discuss the history of the problem, starting with Bishop's original theorem from 1963.
										        </p>
												
												<p>
													<span class="heavy">Keywords:</span> Differential geometry, volume comparison, Lorentz manifolds
												</p>

										        <p>
										        	<a class="my-link" href="./papers/thesis.pdf">Link</a>
										        </p>
											</div>
										</div>
									</div>
								</div> <!-- END ENTRY -->
							</div> <!-- END ACCORDION DIV -->

						</div> <!-- END RESEARCH-CONTENT DIV -->

						<div class="my-content" id="teaching-content">
							<div class="content-wrapper">
								<p>This quarter I am teaching Statistics 10 (Introduction to Statistical Reasoning).</p>

								<p>Past teaching assignments:</p>
								<ul>
									<li>Statistics 495A: Teaching College Statistics (Winter 2015)</li>
									<li>Statistics 100A: Introduction to Probability (Spring 2014)</li>
									<li>Statistics 101B: Introduction to Design and Analysis of Experiments (Winter 2014)</li>
									<li>Statistics 102A: Introduction to Computational Statistics with R (Fall 2013)</li>
									<li>PIC 20A: Principles of Java (Spring 2010)</li>
									<li>PIC 10A: Introduction to C++ Programming (Winter 2010)</li>
									<li>PIC 10A: Introduction to C++ Programming (Fall 2009)</li>
								</ul>
							</div>
						</div> <!-- END TEACHING-CONTENT DIV -->

						<div class="my-content" id="software-content">
							<p class="my-hidden-heading">// software</p>
							<div class="content-wrapper">
								<p>Under construction.</p>
							</div>
						</div> <!-- END SOFTWARE-CONTENT DIV -->

						<div class="my-content" id="consulting-content">
							<p class="my-hidden-heading">// consulting</p>
							<div class="content-wrapper">
								<p>I am available for part-time and short-term consulting projects in analytics, web development, graphic design, modeling and simulation. I am experienced in many core languages as well as web-oriented and design-oriented software.</p>

								<ul>
									<li>Data: R + Rcpp / SQL / PL/pgSQL / MapReduce</li>
									<li>Numerics: Mathematica / MATLAB</li>
									<li>Core: C++ / C / Python / Java / Visual Basic</li>
									<li><span id="playtime">Web: HTML(5) / XML / CSS / Javascript / jQuery</span></li>
									<li>Design: Photoshop / Digital photography / Video editing</li>
								</ul>

								<p>I am particularly interested in the analysis of social data, including social networks, advertising, and marketing.</p>
							</div>

				            <p class="my-subject-heading">// tutoring</p>
				            <div class="content-wrapper">
								<p>I am also available for tutoring. I have over 10 years experience and specialize in working with students of all ages with exceptional difficulties in mathematics. You may view my resum&#233; <a class="my-link" href="./bryonresume.pdf">here</a>.
								</p>
							</div>

				            <p class="my-subject-heading">// availability</p>
				            <div class="content-wrapper">
								<p>Please <a class="my-link" id="click-about-2" href="#">e-mail me</a> for more details.</p>
							</div>
						</div> <!-- END CONSULTING-CONTENT DIV -->

						<div class="my-content" id="about-content">
							<p class="my-hidden-heading">// contact</p>
							<div class="content-wrapper">
								<p>If you want to...</p>

								<p>...e-mail me: <a class="my-link" href="#">bryon</a> at <a class="my-link" href="#">stat</a> dot <a class="my-link" href="#">ucla</a> dot <a class="my-link" href="#">edu</a> .</p>

								<p>...find me on LinkedIn, <a class="my-link" href="https://www.linkedin.com/in/bryonaragam/">click here.</a></p>

								<p>...see my resum&#233;, <a class="my-link" href="./bryonresume.pdf">click here.</a></p>

								<p>I also got a little bored while <a class="my-link" href="#" data-toggle="modal" data-target="#design-modal">designing</a> this site so I hid some easter eggs here and there.</p>
							</div>

							<p class="my-subject-heading">// more about me</p>
							<div class="content-wrapper">
								<p>I am a lecturer in the Department of Statistics at UCLA. I recently received my PhD in Statistics from UCLA (2015), where I studied graphical models and high-dimensional statistics under Professor Qing Zhou. I also have a MA in Applied Math from UCLA and a BS in Math from the University of Tennessee. Once upon a time, I had a short stint doing web development and analytics for a small market research firm. This explains my obsession with marketing and advertising.</p>
							</div>

							<!-- Modal -->
							<div class="modal fade" id="design-modal" tabindex="-1" role="dialog" aria-labelledby="design-modal-title" aria-hidden="true">
								<div class="modal-dialog">
									<div class="modal-content">
										<div class="modal-header">
											<button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
											
											<span class="modal-title force-text-colour my-subject-heading" id="design-modal-title"> // About this site</span>
										</div>

										<div class="modal-body force-text-colour">
											<p class="force-text-colour">This site was designed using...</p>

												<ul>
													<li>Bootstrap 3</li>
													<li>jQuery</li>
													<li>LESS + CSS3</li>
													<li><a class="my-link" href="http://liliansurya.carbonmade.com/">Lilian Surya</a></li>
												</ul>

											<p class="force-text-colour">No images were used or harmed in the development of this site.</p>

											<p class="force-text-colour">If you are having any issues navigating the site or (gasp!) found a bug, please let me know by shooting me an e-mail at <a class="my-link" href="#">contactbryon@gmail.com</a>.</p>										
										</div>
									</div>
								</div>
							</div> <!-- END MODAL DIV -->

						</div> <!-- END ABOUT-CONTENT DIV -->

					</div> <!-- END MY-MAIN-CONTENT DIV -->
					
				</div> <!-- END ROW #2 -->

			</div> <!-- END BG-LAYER CONTAINER -->
			
		</div> <!-- END MAIN CONTAINER -->
		
		<!-- MORE OF MY JAVASCRIPT (LOAD THE INTERACTIVE STUFF LAST) -->
		<script src="main.js"></script>
		<script src="drags-dbl.js"></script>
	</body>
	
</html>